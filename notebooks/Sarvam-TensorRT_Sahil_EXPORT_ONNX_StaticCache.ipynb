{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERT TINYLLAMA MODEL TO ONNX WITH TOKEN-WISE DECODING\n",
    "Kaggle Notebook [Link](https://www.kaggle.com/code/sahil112/tensorrt-v3/notebook?scriptVersionId=207733517). Please check Version 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-16T08:20:25.151774Z",
     "iopub.status.busy": "2024-11-16T08:20:25.151261Z",
     "iopub.status.idle": "2024-11-16T08:20:25.158542Z",
     "shell.execute_reply": "2024-11-16T08:20:25.157210Z",
     "shell.execute_reply.started": "2024-11-16T08:20:25.151729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input/'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:20:25.165215Z",
     "iopub.status.busy": "2024-11-16T08:20:25.164625Z",
     "iopub.status.idle": "2024-11-16T08:20:42.304038Z",
     "shell.execute_reply": "2024-11-16T08:20:42.302406Z",
     "shell.execute_reply.started": "2024-11-16T08:20:25.165133Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorrt in /opt/conda/lib/python3.10/site-packages (10.6.0)\n",
      "Requirement already satisfied: onnxruntime in /opt/conda/lib/python3.10/site-packages (1.20.0)\n",
      "Requirement already satisfied: tensorrt-cu12==10.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorrt) (10.6.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (21.3)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (3.20.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->onnxruntime) (3.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Installed to a local directory and now I keep adding the notebook to this work\n",
    "\n",
    "!pip install tensorrt onnxruntime # --quiet\n",
    "# !pip install --upgrade tensorrt onnxruntime --target=/kaggle/working/mysitepackages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:20:42.307727Z",
     "iopub.status.busy": "2024-11-16T08:20:42.307212Z",
     "iopub.status.idle": "2024-11-16T08:20:42.316331Z",
     "shell.execute_reply": "2024-11-16T08:20:42.314630Z",
     "shell.execute_reply.started": "2024-11-16T08:20:42.307678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import transformers\n",
    "# from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.onnx import OnnxConfig\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:20:42.318498Z",
     "iopub.status.busy": "2024-11-16T08:20:42.318002Z",
     "iopub.status.idle": "2024-11-16T08:20:42.333772Z",
     "shell.execute_reply": "2024-11-16T08:20:42.332386Z",
     "shell.execute_reply.started": "2024-11-16T08:20:42.318432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covert to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:20:42.335740Z",
     "iopub.status.busy": "2024-11-16T08:20:42.335297Z",
     "iopub.status.idle": "2024-11-16T08:20:44.043060Z",
     "shell.execute_reply": "2024-11-16T08:20:44.041388Z",
     "shell.execute_reply.started": "2024-11-16T08:20:42.335698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Important to do this step in Kaggle only while creating :/\n",
    "! touch tiny_llama.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:20:44.048907Z",
     "iopub.status.busy": "2024-11-16T08:20:44.048417Z",
     "iopub.status.idle": "2024-11-16T08:20:44.836087Z",
     "shell.execute_reply": "2024-11-16T08:20:44.834604Z",
     "shell.execute_reply.started": "2024-11-16T08:20:44.048857Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "device='cpu' # 'cuda'\n",
    "\n",
    "model_path = \"/kaggle/input/m/mambagetout/tinyllama/pytorch/default/1/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token # added because of generate thing\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:20:44.838177Z",
     "iopub.status.busy": "2024-11-16T08:20:44.837755Z",
     "iopub.status.idle": "2024-11-16T08:20:44.844036Z",
     "shell.execute_reply": "2024-11-16T08:20:44.842618Z",
     "shell.execute_reply.started": "2024-11-16T08:20:44.838135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import StaticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:20:44.847294Z",
     "iopub.status.busy": "2024-11-16T08:20:44.846031Z",
     "iopub.status.idle": "2024-11-16T08:20:44.854891Z",
     "shell.execute_reply": "2024-11-16T08:20:44.853459Z",
     "shell.execute_reply.started": "2024-11-16T08:20:44.847213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create dummy inputs\n",
    "batch_size = 1\n",
    "sequence_length = 256  # Reduced for testing\n",
    "\n",
    "# Get model configuration\n",
    "num_layers = model.config.num_hidden_layers\n",
    "num_attention_heads = model.config.num_attention_heads\n",
    "hidden_size = model.config.hidden_size\n",
    "head_dim = hidden_size // num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:20:44.857320Z",
     "iopub.status.busy": "2024-11-16T08:20:44.856840Z",
     "iopub.status.idle": "2024-11-16T08:20:44.879038Z",
     "shell.execute_reply": "2024-11-16T08:20:44.877805Z",
     "shell.execute_reply.started": "2024-11-16T08:20:44.857278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, position_ids):\n",
    "        # Initialize static cache\n",
    "        past_key_values = StaticCache(\n",
    "            config=self.model.config,\n",
    "            batch_size=batch_size,\n",
    "            max_cache_len= sequence_length, # 32,  # Match the sequence length\n",
    "            device=input_ids.device,\n",
    "            dtype=self.model.dtype\n",
    "        )\n",
    "        \n",
    "        # Generate cache position\n",
    "        cache_position = torch.arange(sequence_length, device=input_ids.device)\n",
    "        \n",
    "        # Forward pass with static cache\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            cache_position=cache_position, # Can keep None also? TRYING FOR NOW\n",
    "            use_cache=True,\n",
    "            return_dict=False\n",
    "        )\n",
    "        \n",
    "        return outputs[0]  # Return only logits for simplicity\n",
    "\n",
    "# Wrap the model, keeping on gpu is essestial else \n",
    "# error two devices found cuda and cuda:0 error\n",
    "# wrapped_model = ModelWrapper(model).cuda()\n",
    "wrapped_model = ModelWrapper(model).to(device)\n",
    "\n",
    "# Input tensors\n",
    "dummy_input_ids = torch.ones(batch_size, sequence_length, dtype=torch.long, device=device)\n",
    "dummy_attention_mask = torch.ones(batch_size, sequence_length, dtype=torch.long, device=device)\n",
    "dummy_position_ids = torch.arange(sequence_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "# Define dynamic axes for variable sequence lengths\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'position_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:20:44.881609Z",
     "iopub.status.busy": "2024-11-16T08:20:44.881124Z",
     "iopub.status.idle": "2024-11-16T08:20:44.888095Z",
     "shell.execute_reply": "2024-11-16T08:20:44.886638Z",
     "shell.execute_reply.started": "2024-11-16T08:20:44.881565Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # The previous model can be exported with dynamic shapes\n",
    "# export_options = torch.onnx.ExportOptions(dynamic_shapes=True)\n",
    "# onnx_program = torch.onnx.dynamo_export(\n",
    "#     model, \n",
    "#     *args, \n",
    "#     **kwargs, \n",
    "#     export_options=export_options\n",
    "# )\n",
    "# onnx_program.save(\"/kaggle/working/tiny_llama.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:20:44.890091Z",
     "iopub.status.busy": "2024-11-16T08:20:44.889681Z",
     "iopub.status.idle": "2024-11-16T08:22:03.378948Z",
     "shell.execute_reply": "2024-11-16T08:22:03.377489Z",
     "shell.execute_reply.started": "2024-11-16T08:20:44.890049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:97: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    }
   ],
   "source": [
    "# Export the model\n",
    "\n",
    "# if using cpu\n",
    "torch.onnx.export(\n",
    "    wrapped_model,\n",
    "    (\n",
    "        dummy_input_ids,\n",
    "        dummy_attention_mask,\n",
    "        dummy_position_ids,\n",
    "    ),\n",
    "    '/kaggle/working/tiny_llama.onnx',\n",
    "    input_names=['input_ids', 'attention_mask', 'position_ids'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=18, # DO NOT, I REPEAT DO NOT USE OPSET_VERSION 15. FULL PAIN IT ISSSS.\n",
    "    do_constant_folding=False, # True, # CHECKING ANOTHER OPTION because later error\n",
    "    verbose=False \n",
    ")\n",
    "\n",
    "# if using gpu\n",
    "# with torch.no_grad():\n",
    "#     torch.onnx.export(\n",
    "#         wrapped_model,\n",
    "#         (\n",
    "#             dummy_input_ids,\n",
    "#             dummy_attention_mask,\n",
    "#             dummy_position_ids,\n",
    "#         ),\n",
    "#         '/kaggle/working/tiny_llama.onnx',\n",
    "#         input_names=['input_ids', 'attention_mask', 'position_ids'],\n",
    "#         output_names=['logits'],\n",
    "#         dynamic_axes=dynamic_axes,\n",
    "#         opset_version=15,\n",
    "#         do_constant_folding=True,\n",
    "#         verbose=False\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **load the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:22:03.384895Z",
     "iopub.status.busy": "2024-11-16T08:22:03.384350Z",
     "iopub.status.idle": "2024-11-16T08:22:03.394472Z",
     "shell.execute_reply": "2024-11-16T08:22:03.393223Z",
     "shell.execute_reply.started": "2024-11-16T08:22:03.384848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/tiny_llama.onnx\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        if '.onnx' in filename:\n",
    "            print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:22:03.396541Z",
     "iopub.status.busy": "2024-11-16T08:22:03.396087Z",
     "iopub.status.idle": "2024-11-16T08:22:29.238609Z",
     "shell.execute_reply": "2024-11-16T08:22:29.236423Z",
     "shell.execute_reply.started": "2024-11-16T08:22:03.396498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"/kaggle/working/tiny_llama.onnx\")\n",
    "# onnx_model = onnx.load(\"/kaggle/working/tiny_llama_basic.onnx\")\n",
    "# onnx_model = onnx.load(\"/kaggle/input/tensorrt-v3/tiny_llama.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:22:29.241771Z",
     "iopub.status.busy": "2024-11-16T08:22:29.241335Z",
     "iopub.status.idle": "2024-11-16T08:22:29.251581Z",
     "shell.execute_reply": "2024-11-16T08:22:29.250317Z",
     "shell.execute_reply.started": "2024-11-16T08:22:29.241726Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:22:29.260011Z",
     "iopub.status.busy": "2024-11-16T08:22:29.259471Z",
     "iopub.status.idle": "2024-11-16T08:22:29.267749Z",
     "shell.execute_reply": "2024-11-16T08:22:29.266511Z",
     "shell.execute_reply.started": "2024-11-16T08:22:29.259962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# onnx.checker.check_model(onnx_model)\n",
    "# Gives ValidationError: The model does not have an ir_version set properly.\n",
    "# Fine only, mostly a bug\n",
    "\n",
    "# Also might give\n",
    "# ValueError: Message onnx.ModelProto exceeds maximum protobuf size of 2GB: 4402590179"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference using the ort InferenceSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:22:29.270141Z",
     "iopub.status.busy": "2024-11-16T08:22:29.269554Z",
     "iopub.status.idle": "2024-11-16T08:23:19.587515Z",
     "shell.execute_reply": "2024-11-16T08:23:19.585812Z",
     "shell.execute_reply.started": "2024-11-16T08:22:29.270061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(\"/kaggle/working/tiny_llama.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "# ort_session = ort.InferenceSession(\"/kaggle/input/tensorrt-v3/tiny_llama.onnx\", providers=[\"CPUExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:23:19.593333Z",
     "iopub.status.busy": "2024-11-16T08:23:19.592810Z",
     "iopub.status.idle": "2024-11-16T08:23:24.507277Z",
     "shell.execute_reply": "2024-11-16T08:23:24.505961Z",
     "shell.execute_reply.started": "2024-11-16T08:23:19.593279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "ort_inputs = {\n",
    "    ort_session.get_inputs()[0].name: to_numpy(dummy_input_ids),\n",
    "    ort_session.get_inputs()[1].name: to_numpy(dummy_attention_mask),\n",
    "    ort_session.get_inputs()[2].name: to_numpy(dummy_position_ids)\n",
    "}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:23:24.509617Z",
     "iopub.status.busy": "2024-11-16T08:23:24.509214Z",
     "iopub.status.idle": "2024-11-16T08:23:24.518308Z",
     "shell.execute_reply": "2024-11-16T08:23:24.517023Z",
     "shell.execute_reply.started": "2024-11-16T08:23:24.509576Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 1, (1, 256, 32000))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ort_outs), len(ort_outs), ort_outs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:23:24.520241Z",
     "iopub.status.busy": "2024-11-16T08:23:24.519802Z",
     "iopub.status.idle": "2024-11-16T08:24:07.704410Z",
     "shell.execute_reply": "2024-11-16T08:24:07.703196Z",
     "shell.execute_reply.started": "2024-11-16T08:23:24.520199Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# check if outputs are close\n",
    "np.testing.assert_allclose(\n",
    "    to_numpy(wrapped_model(dummy_input_ids, dummy_attention_mask, dummy_position_ids)), \n",
    "    ort_outs[0], \n",
    "    rtol=1e-03, \n",
    "    atol=1e-04\n",
    ")\n",
    "# throws assertion error when atol is 1e-05\n",
    "# So I think this is fair enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this works, so now work with text inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:07.706302Z",
     "iopub.status.busy": "2024-11-16T08:24:07.705899Z",
     "iopub.status.idle": "2024-11-16T08:24:07.723673Z",
     "shell.execute_reply": "2024-11-16T08:24:07.722085Z",
     "shell.execute_reply.started": "2024-11-16T08:24:07.706259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_input_ids = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) \n",
      " dummy_attention_mask = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) \n",
      " dummy_position_ids = tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "         168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "         182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "         210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "         224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "         252, 253, 254, 255]])\n"
     ]
    }
   ],
   "source": [
    "print(f'dummy_input_ids = {dummy_input_ids} \\n dummy_attention_mask = {dummy_attention_mask} \\n dummy_position_ids = {dummy_position_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:07.725926Z",
     "iopub.status.busy": "2024-11-16T08:24:07.725525Z",
     "iopub.status.idle": "2024-11-16T08:24:07.737278Z",
     "shell.execute_reply": "2024-11-16T08:24:07.735959Z",
     "shell.execute_reply.started": "2024-11-16T08:24:07.725883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_inputs(text_input, tokenizer, target_length=256, device='cuda'):\n",
    "    \"\"\"\n",
    "    Prepare inputs by padding/truncating to target length\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    encoded = tokenizer.encode(text_input, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = encoded[0]  # Remove batch dimension\n",
    "    \n",
    "    # Get the actual sequence length\n",
    "    seq_len = len(input_ids)\n",
    "    \n",
    "    if seq_len > target_length:\n",
    "        # Truncate if longer than target_length\n",
    "        input_ids = input_ids[:target_length]\n",
    "        seq_len = target_length\n",
    "    \n",
    "    # Pad input_ids\n",
    "    padding_length = target_length - seq_len\n",
    "    padding_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    input_ids = torch.cat([\n",
    "        input_ids,\n",
    "        torch.full((padding_length,), padding_id, dtype=torch.long)\n",
    "    ])\n",
    "    \n",
    "    # Create attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = torch.cat([\n",
    "        torch.ones(seq_len, dtype=torch.long),  # Explicitly set dtype to long\n",
    "        torch.zeros(padding_length, dtype=torch.long)  # Explicitly set dtype to long\n",
    "    ])\n",
    "    \n",
    "    # Create position ids\n",
    "    # position_ids = torch.arange(target_length)\n",
    "    position_ids = torch.arange(target_length, dtype=torch.long)  # Explicitly set dtype to long\n",
    "\n",
    "    \n",
    "    # Add batch dimension and move to device\n",
    "    input_ids = input_ids.unsqueeze(0).to(device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(device)\n",
    "    position_ids = position_ids.unsqueeze(0).to(device)\n",
    "    \n",
    "    return input_ids, attention_mask, position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:07.739801Z",
     "iopub.status.busy": "2024-11-16T08:24:07.739245Z",
     "iopub.status.idle": "2024-11-16T08:24:07.755956Z",
     "shell.execute_reply": "2024-11-16T08:24:07.754686Z",
     "shell.execute_reply.started": "2024-11-16T08:24:07.739744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_both_inferences(text_input: str):\n",
    "\n",
    "    input_ids, attention_mask, position_ids = prepare_inputs(\n",
    "        text_input, \n",
    "        tokenizer, \n",
    "        target_length=256,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    wrapped_model_outputs = to_numpy(wrapped_model(input_ids, attention_mask, position_ids))\n",
    "\n",
    "    ort_inputs = { \n",
    "        ort_session.get_inputs()[0].name: to_numpy(input_ids),\n",
    "        ort_session.get_inputs()[1].name: to_numpy(attention_mask),\n",
    "        ort_session.get_inputs()[2].name: to_numpy(position_ids)\n",
    "    }\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "    return wrapped_model_outputs, ort_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:07.757992Z",
     "iopub.status.busy": "2024-11-16T08:24:07.757569Z",
     "iopub.status.idle": "2024-11-16T08:24:22.057856Z",
     "shell.execute_reply": "2024-11-16T08:24:22.056234Z",
     "shell.execute_reply.started": "2024-11-16T08:24:07.757951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wrapped_model_outputs, ort_outs = make_both_inferences(text_input ='Describe Albert Einstein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:22.061068Z",
     "iopub.status.busy": "2024-11-16T08:24:22.060207Z",
     "iopub.status.idle": "2024-11-16T08:24:22.071987Z",
     "shell.execute_reply": "2024-11-16T08:24:22.070906Z",
     "shell.execute_reply.started": "2024-11-16T08:24:22.061009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[-7.396678  , -7.3966794 ,  8.475743  , ..., -4.358731  ,\n",
       "          -7.0164227 , -0.75330424],\n",
       "         [-6.6988673 , -6.6988735 ,  8.643362  , ..., -4.265688  ,\n",
       "          -5.4994435 , -0.25938272],\n",
       "         [-7.3818617 , -7.3818617 , 10.710121  , ..., -4.5778985 ,\n",
       "          -5.3154926 , -1.0604138 ],\n",
       "         ...,\n",
       "         [-4.5339212 , -4.5339227 , 12.72688   , ..., -2.754523  ,\n",
       "          -3.4932256 ,  0.8564618 ],\n",
       "         [-4.582982  , -4.582982  , 12.599232  , ..., -2.5835834 ,\n",
       "          -3.1678545 ,  0.604913  ],\n",
       "         [-3.1293983 , -3.1294005 , 12.985866  , ..., -1.3228996 ,\n",
       "          -1.9311588 ,  1.525508  ]]], dtype=float32),\n",
       " [array([[[-7.3966737 , -7.3966737 ,  8.475735  , ..., -4.3587313 ,\n",
       "           -7.01641   , -0.7532996 ],\n",
       "          [-6.698857  , -6.698863  ,  8.643356  , ..., -4.2656827 ,\n",
       "           -5.49943   , -0.2593714 ],\n",
       "          [-7.381863  , -7.3818655 , 10.7101145 , ..., -4.5778995 ,\n",
       "           -5.3154883 , -1.060413  ],\n",
       "          ...,\n",
       "          [-4.533912  , -4.533914  , 12.726867  , ..., -2.7545178 ,\n",
       "           -3.4932184 ,  0.8564713 ],\n",
       "          [-4.5829725 , -4.5829735 , 12.599226  , ..., -2.583576  ,\n",
       "           -3.1678467 ,  0.60491866],\n",
       "          [-3.1293988 , -3.1294007 , 12.985844  , ..., -1.3228956 ,\n",
       "           -1.9311583 ,  1.5255063 ]]], dtype=float32)])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_model_outputs, ort_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:22.074174Z",
     "iopub.status.busy": "2024-11-16T08:24:22.073348Z",
     "iopub.status.idle": "2024-11-16T08:24:22.085229Z",
     "shell.execute_reply": "2024-11-16T08:24:22.083846Z",
     "shell.execute_reply.started": "2024-11-16T08:24:22.074124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# NOTE: ALL ARE STILL SINGLE BATCH SIZE, IT DOESNT SUPPORT LARGER BATCH SIZE YET\n",
    "# model_op_new = Out[80][0]\n",
    "# model_op_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The token ids converted back to text aren't beautiful though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:22.087443Z",
     "iopub.status.busy": "2024-11-16T08:24:22.086901Z",
     "iopub.status.idle": "2024-11-16T08:24:22.140359Z",
     "shell.execute_reply": "2024-11-16T08:24:22.139141Z",
     "shell.execute_reply.started": "2024-11-16T08:24:22.087382Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amedamed The And Be.ededed And And Andeded\\n A A A A\\n\\n\\n\\n And And\\n\\n\\n M And And And And And\\n\\n And And And And And And And\\n\\n\\n And And A A\\n\\n\\n\\n And Andamed\\n\\n\\n\\n\\n And And And And And And And And And And And And And And\\n\\n The A A\\n\\n\\n The M And And And\\n\\n\\n\\n\\n\\n\\n\\n And And And\\n\\n The The M A A A\\n\\n And And And And And And And And And And And And And And And And\\n\\n\\n And And And And\\n\\n The The A A A\\n\\n\\n\\n\\n\\n And M M A Aed\\n And A And Andamed\\n The M M A A A\\n And And And And And And And And And And And\\n\\n The A A Aed  M M And And\\n\\n\\n\\n\\n\\n\\n\\n And And And And And And And And And And And And And And And And And And And And And And And And And And And And And\\n\\n\\n\\n And And And And\\n\\n The   A A A M ededed And And And And And\\n\\n And And',\n",
       " 'amedamed The And Be.ededed And And Andeded\\n A A A A\\n\\n\\n\\n And And\\n\\n\\n M And And And And And\\n\\n And And And And And And And\\n\\n\\n And And A A\\n\\n\\n\\n And Andamed\\n\\n\\n\\n\\n And And And And And And And And And And And And And And\\n\\n The A A\\n\\n\\n The M And And And\\n\\n\\n\\n\\n\\n\\n\\n And And And\\n\\n The The M A A A\\n\\n And And And And And And And And And And And And And And And And\\n\\n\\n And And And And\\n\\n The The A A A\\n\\n\\n\\n\\n\\n And M M A Aed\\n And A And Andamed\\n The M M A A A\\n And And And And And And And And And And And\\n\\n The A A Aed  M M And And\\n\\n\\n\\n\\n\\n\\n\\n And And And And And And And And And And And And And And And And And And And And And And And And And And And And And\\n\\n\\n\\n And And And And\\n\\n The   A A A M ededed And And And And And\\n\\n And And')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Convert logits to token IDs (if necessary)\n",
    "wrapped_token_ids = torch.argmax(torch.from_numpy(wrapped_model_outputs), dim=-1)\n",
    "ort_token_ids = torch.argmax(torch.from_numpy(ort_outs[0]), dim=-1)  # Assuming first output is relevant\n",
    "\n",
    "# Decode token IDs back to text\n",
    "wrapped_text_output = tokenizer.decode(wrapped_token_ids[0], skip_special_tokens=True)\n",
    "ort_text_output = tokenizer.decode(ort_token_ids[0], skip_special_tokens=True)\n",
    "wrapped_text_output, ort_text_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Decode token by token\n",
    "ref https://github.com/huggingface/transformers/issues/30670"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:22.143132Z",
     "iopub.status.busy": "2024-11-16T08:24:22.142350Z",
     "iopub.status.idle": "2024-11-16T08:24:22.149460Z",
     "shell.execute_reply": "2024-11-16T08:24:22.148272Z",
     "shell.execute_reply.started": "2024-11-16T08:24:22.143054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import StaticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:22.152094Z",
     "iopub.status.busy": "2024-11-16T08:24:22.151552Z",
     "iopub.status.idle": "2024-11-16T08:24:22.160973Z",
     "shell.execute_reply": "2024-11-16T08:24:22.159587Z",
     "shell.execute_reply.started": "2024-11-16T08:24:22.152037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch_device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:22.163114Z",
     "iopub.status.busy": "2024-11-16T08:24:22.162625Z",
     "iopub.status.idle": "2024-11-16T08:24:22.189087Z",
     "shell.execute_reply": "2024-11-16T08:24:22.187509Z",
     "shell.execute_reply.started": "2024-11-16T08:24:22.163057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# use staticCache from huggingface\n",
    "# ref https://github.com/huggingface/transformers/issues/30670\n",
    "\n",
    "def decode_one_tokens(model, cur_token, input_pos, attention_mask, cache_position, past_key_values):\n",
    "    logits = model(\n",
    "        cur_token,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=input_pos,\n",
    "        cache_position=cache_position,\n",
    "        past_key_values=past_key_values,\n",
    "        return_dict=False,\n",
    "        use_cache=True\n",
    "    )[0]\n",
    "    new_token = torch.argmax(logits[:, -1], dim=-1)[:, None]\n",
    "    return new_token\n",
    "\n",
    "\n",
    "def generate(\n",
    "    prompts: List[str], \n",
    "    model, \n",
    "    tokenizer, \n",
    "    num_tokens_to_generate: int = 20 # changed from 40\n",
    ") ->  List[str]:\n",
    "    \n",
    "    global decode_one_tokens\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    batch_size, seq_length = inputs[\"input_ids\"].shape\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        past_key_values = StaticCache(\n",
    "            config=model.config, \n",
    "            max_batch_size=batch_size, \n",
    "            max_cache_len=256,  # changed from 4096\n",
    "            device=torch_device, \n",
    "            dtype=model.dtype\n",
    "        )\n",
    "        \n",
    "        cache_position = torch.arange(seq_length, device=torch_device)\n",
    "        generated_ids = torch.zeros(\n",
    "            batch_size, seq_length + num_tokens_to_generate + 1, dtype=torch.int, device=torch_device\n",
    "        )\n",
    "\n",
    "        print(f'generated pask key values ...')\n",
    "        \n",
    "        generated_ids[:, cache_position] = inputs[\"input_ids\"].to(torch_device).to(torch.int)\n",
    "        print(f'generated generated_ids till cache position ...')\n",
    "        \n",
    "        with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True): # apparently this is outdated now\n",
    "        # with torch.nn.attention.sdpa_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True):\n",
    "            logits = model(\n",
    "                **inputs, cache_position=cache_position, past_key_values=past_key_values, return_dict=False, use_cache=True\n",
    "            )[0]\n",
    "\n",
    "        print(f'got logits from model ...')\n",
    "        \n",
    "        next_token = torch.argmax(logits[:, -1], dim=-1)[:, None]\n",
    "        generated_ids[:, seq_length] = next_token[:, 0]\n",
    "\n",
    "        # Not using torch.compile to simplify debugging\n",
    "        # decode_one_tokens = torch.compile(decode_one_tokens, mode=\"reduce-overhead\", fullgraph=True)\n",
    "        cache_position = torch.tensor([seq_length + 1], device=torch_device)\n",
    "        attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1) # update and pass every step\n",
    "\n",
    "        print(f'got cache position and attention mask ...')\n",
    "\n",
    "        for _ in range(1, num_tokens_to_generate):\n",
    "            with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True):\n",
    "                next_token = decode_one_tokens(\n",
    "                    model = model, \n",
    "                    cur_token = next_token.clone(), \n",
    "                    input_pos = None, \n",
    "                    attention_mask = attention_mask, \n",
    "                    cache_position = cache_position, \n",
    "                    past_key_values = past_key_values\n",
    "                )\n",
    "                \n",
    "                print(f'generated {_} next token in loop')\n",
    "                generated_ids[:, cache_position] = next_token.int()\n",
    "                \n",
    "            cache_position += 1\n",
    "            # position_ids = position_ids[:, -1:] + 1 # variable referenced before assignment\n",
    "            attention_mask = torch.cat(\n",
    "                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], \n",
    "                dim=-1\n",
    "            )\n",
    "            print(f'\\t cache posn, posn id, attn mask etc has been updated in loop ')\n",
    "\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:22.191871Z",
     "iopub.status.busy": "2024-11-16T08:24:22.191308Z",
     "iopub.status.idle": "2024-11-16T08:24:34.073434Z",
     "shell.execute_reply": "2024-11-16T08:24:34.072071Z",
     "shell.execute_reply.started": "2024-11-16T08:24:22.191811Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated pask key values ...\n",
      "generated generated_ids till cache position ...\n",
      "got logits from model ...\n",
      "got cache position and attention mask ...\n",
      "generated 1 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 2 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 3 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 4 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 5 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 6 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 7 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 8 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 9 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 10 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 11 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 12 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 13 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 14 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 15 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 16 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 17 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 18 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n",
      "generated 19 next token in loop\n",
      "\t cache posn, posn id, attn mask etc has been updated in loop \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Describe Albert Einstein  2019.\\nThe 2019 edition of the annual event will be',\n",
       " \"How does the sun burn? 2019\\nThe 2019 edition of the World Economic Forum's\"]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    prompts = [\"Describe Albert Einstein \", \"How does the sun burn? \"], \n",
    "    model = model , # wrapped_model,\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# PART 3\n",
    "Try with the generate method directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:34.076236Z",
     "iopub.status.busy": "2024-11-16T08:24:34.075434Z",
     "iopub.status.idle": "2024-11-16T08:24:34.082769Z",
     "shell.execute_reply": "2024-11-16T08:24:34.081542Z",
     "shell.execute_reply.started": "2024-11-16T08:24:34.076161Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:34.084660Z",
     "iopub.status.busy": "2024-11-16T08:24:34.084243Z",
     "iopub.status.idle": "2024-11-16T08:24:34.647345Z",
     "shell.execute_reply": "2024-11-16T08:24:34.645875Z",
     "shell.execute_reply.started": "2024-11-16T08:24:34.084617Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "device='cpu' # 'cuda'\n",
    "\n",
    "model_path = \"/kaggle/input/m/mambagetout/tinyllama/pytorch/default/1/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token # added because of generate thing\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:34.649469Z",
     "iopub.status.busy": "2024-11-16T08:24:34.648991Z",
     "iopub.status.idle": "2024-11-16T08:24:34.655570Z",
     "shell.execute_reply": "2024-11-16T08:24:34.654076Z",
     "shell.execute_reply.started": "2024-11-16T08:24:34.649415Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-11-16T08:24:34.657872Z",
     "iopub.status.busy": "2024-11-16T08:24:34.657354Z",
     "iopub.status.idle": "2024-11-16T08:24:39.140874Z",
     "shell.execute_reply": "2024-11-16T08:24:39.139520Z",
     "shell.execute_reply.started": "2024-11-16T08:24:34.657808Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created the past key values ... \n",
      "['The theory of special relativity states 2.0.0.0.0.0.0.0.']\n"
     ]
    }
   ],
   "source": [
    "# model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "# If not done torch.compile, then we don't get the error\n",
    "\n",
    "input_text = \"The theory of special relativity states \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "prompt_length = input_ids.input_ids.shape[1]\n",
    "model.generation_config.max_new_tokens = 16\n",
    "\n",
    "past_key_values = StaticCache(\n",
    "    config=model.config,\n",
    "    batch_size=1,\n",
    "    # If you plan to reuse the cache, make sure the cache length is large enough for all cases\n",
    "    max_cache_len=prompt_length+(model.generation_config.max_new_tokens*2),\n",
    "    device=model.device,\n",
    "    dtype=model.dtype\n",
    ")\n",
    "\n",
    "print(f'created the past key values ... ')\n",
    "outputs = model.generate(\n",
    "    **input_ids, \n",
    "    # past_key_values=past_key_values\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "# ['The theory of special relativity states 1. The speed of light is constant in all inertial reference frames. 2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output with and without past key values is not the same!**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 207202020,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 156963,
     "modelInstanceId": 134194,
     "sourceId": 157908,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
