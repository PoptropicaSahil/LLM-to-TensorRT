{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I have converted a tinyllama model to onnx format like this. Your tasks are the following - \n",
    "1. Check the code below and point out if there are inaccuracies. I want a static cache and support for dynamic inputs and then move forward to converting the model to onnx\n",
    "2. Once you have validated the onnx conversion ,I want to get the model's outputs. I have shared my code below to get outputs from the onnx model, but the outputs aren't normal, they are garbled. I am hence sharing my current inference code and a reference code that I want you to follow/improvise upon such that I can now try the token by token generation. \n",
    "\n",
    "<<  Code to wrap the model with static cache etc>>\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, position_ids):\n",
    "        # Initialize static cache\n",
    "        past_key_values = StaticCache(\n",
    "            config=self.model.config,\n",
    "            batch_size=batch_size,\n",
    "            max_cache_len= sequence_length, # 32,  # Match the sequence length\n",
    "            device=input_ids.device,\n",
    "            dtype=self.model.dtype\n",
    "        )\n",
    "        \n",
    "        # Generate cache position\n",
    "        cache_position = torch.arange(sequence_length, device=input_ids.device)\n",
    "        \n",
    "        # Forward pass with static cache\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=True,\n",
    "            return_dict=False\n",
    "        )\n",
    "        \n",
    "        return outputs[0]  # Return only logits for simplicity\n",
    "\n",
    "\n",
    "wrapped_model = ModelWrapper(model).to(device)\n",
    "\n",
    "# Input tensors\n",
    "dummy_input_ids = torch.ones(batch_size, sequence_length, dtype=torch.long, device=device)\n",
    "dummy_attention_mask = torch.ones(batch_size, sequence_length, dtype=torch.long, device=device)\n",
    "dummy_position_ids = torch.arange(sequence_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "# Define dynamic axes for variable sequence lengths\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'position_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "}\n",
    "\n",
    "<<  Code to convert the model to onnx >>\n",
    "torch.onnx.export(\n",
    "    wrapped_model,\n",
    "    (\n",
    "        dummy_input_ids,\n",
    "        dummy_attention_mask,\n",
    "        dummy_position_ids,\n",
    "    ),\n",
    "    '/kaggle/working/tiny_llama.onnx',\n",
    "    input_names=['input_ids', 'attention_mask', 'position_ids'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=18, \n",
    "    do_constant_folding=False,\n",
    "    verbose=False \n",
    ")\n",
    "\n",
    "<<My inference code>>\n",
    "ort_session = ort.InferenceSession(\"/kaggle/working/tiny_llama.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "def prepare_inputs(text_input, tokenizer, target_length=256, device='cuda'):\n",
    "    \"\"\"\n",
    "    Prepare inputs by padding/truncating to target length\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    encoded = tokenizer.encode(text_input, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = encoded[0]  # Remove batch dimension\n",
    "    \n",
    "    # Get the actual sequence length\n",
    "    seq_len = len(input_ids)\n",
    "    \n",
    "    if seq_len > target_length:\n",
    "        # Truncate if longer than target_length\n",
    "        input_ids = input_ids[:target_length]\n",
    "        seq_len = target_length\n",
    "    \n",
    "    # Pad input_ids\n",
    "    padding_length = target_length - seq_len\n",
    "    padding_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    input_ids = torch.cat([\n",
    "        input_ids,\n",
    "        torch.full((padding_length,), padding_id, dtype=torch.long)\n",
    "    ])\n",
    "    \n",
    "    # Create attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = torch.cat([\n",
    "        torch.ones(seq_len, dtype=torch.long),  # Explicitly set dtype to long\n",
    "        torch.zeros(padding_length, dtype=torch.long)  # Explicitly set dtype to long\n",
    "    ])\n",
    "    \n",
    "    # Create position ids\n",
    "    # position_ids = torch.arange(target_length)\n",
    "    position_ids = torch.arange(target_length, dtype=torch.long)  # Explicitly set dtype to long\n",
    "\n",
    "    \n",
    "    # Add batch dimension and move to device\n",
    "    input_ids = input_ids.unsqueeze(0).to(device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(device)\n",
    "    position_ids = position_ids.unsqueeze(0).to(device)\n",
    "    \n",
    "    return input_ids, attention_mask, position_ids\n",
    "\n",
    "\n",
    "def make_both_inferences(text_input: str):\n",
    "\n",
    "    input_ids, attention_mask, position_ids = prepare_inputs(\n",
    "        text_input, \n",
    "        tokenizer, \n",
    "        target_length=256,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    wrapped_model_outputs = to_numpy(wrapped_model(input_ids, attention_mask, position_ids))\n",
    "\n",
    "    ort_inputs = { \n",
    "        ort_session.get_inputs()[0].name: to_numpy(input_ids),\n",
    "        ort_session.get_inputs()[1].name: to_numpy(attention_mask),\n",
    "        ort_session.get_inputs()[2].name: to_numpy(position_ids)\n",
    "    }\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "    return wrapped_model_outputs, ort_outs\n",
    "\n",
    "\n",
    "<<  Reference Code that I saw to perform token by token conversion, you have to think along these lines to give me my inference code >>\n",
    "# use staticCache from huggingface\n",
    "# ref https://github.com/huggingface/transformers/issues/30670\n",
    "\n",
    "def decode_one_tokens(model, cur_token, input_pos, attention_mask, cache_position, past_key_values):\n",
    "    logits = model(\n",
    "        cur_token,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=input_pos,\n",
    "        cache_position=cache_position,\n",
    "        past_key_values=past_key_values,\n",
    "        return_dict=False,\n",
    "        use_cache=True\n",
    "    )[0]\n",
    "    new_token = torch.argmax(logits[:, -1], dim=-1)[:, None]\n",
    "    return new_token\n",
    "\n",
    "\n",
    "def generate(\n",
    "    prompts: List[str], \n",
    "    model: LlamaForCausalLM, \n",
    "    tokenizer: LlamaTokenizer, \n",
    "    num_tokens_to_generate: int = 40\n",
    ") ->  List[str]:\n",
    "    \n",
    "    global decode_one_tokens\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    batch_size, seq_length = inputs[\"input_ids\"].shape\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        past_key_values = StaticCache(\n",
    "            config=model.config, \n",
    "            max_batch_size=batch_size, \n",
    "            max_cache_len=4096, device=torch_device, dtype=model.dtype\n",
    "        )\n",
    "        cache_position = torch.arange(seq_length, device=torch_device)\n",
    "        generated_ids = torch.zeros(\n",
    "            batch_size, seq_length + num_tokens_to_generate + 1, dtype=torch.int, device=torch_device\n",
    "        )\n",
    "        generated_ids[:, cache_position] = inputs[\"input_ids\"].to(torch_device).to(torch.int)\n",
    "        with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True):\n",
    "            logits = model(\n",
    "                **inputs, cache_position=cache_position, past_key_values=past_key_values, return_dict=False, use_cache=True\n",
    "            )[0]\n",
    "        next_token = torch.argmax(logits[:, -1], dim=-1)[:, None]\n",
    "        generated_ids[:, seq_length] = next_token[:, 0]\n",
    "\n",
    "        # Not using torch.compile to simplify debugging\n",
    "        # decode_one_tokens = torch.compile(decode_one_tokens, mode=\"reduce-overhead\", fullgraph=True)\n",
    "        cache_position = torch.tensor([seq_length + 1], device=torch_device)\n",
    "        attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1) # update and pass every step\n",
    "        for _ in range(1, num_tokens_to_generate):\n",
    "            with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True):\n",
    "                next_token = decode_one_tokens(model, next_token.clone(), None, attention_mask, cache_position, past_key_values)\n",
    "                generated_ids[:, cache_position] = next_token.int()\n",
    "            cache_position += 1\n",
    "            position_ids = position_ids[:, -1:] + 1\n",
    "            attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\n",
    "\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's address your tasks step-by-step:\n",
    "\n",
    "1. Checking the ONNX Conversion Code\n",
    "Issues in the Model Wrapper:\n",
    "Static Cache Initialization: The StaticCache class is not defined in your provided code. Ensure it's correctly imported or defined.\n",
    "Cache Position: The cache_position should be initialized correctly for each step during inference. In the provided wrapper, it seems like cache_position is fixed, which won't work for dynamic generation.\n",
    "Batch Size and Sequence Length: These should be dynamic, but in the wrapper, they are hardcoded. You need to handle them dynamically.\n",
    "Revised Model Wrapper:\n",
    "\n",
    "import torch\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, position_ids, past_key_values=None, cache_position=None):\n",
    "        # Forward pass with dynamic cache\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=True,\n",
    "            return_dict=False\n",
    "        )\n",
    "        \n",
    "        return outputs[0], outputs[1]  # Return logits and updated past_key_values\n",
    "\n",
    "# Assuming StaticCache is defined somewhere\n",
    "wrapped_model = ModelWrapper(model).to(device)\n",
    "\n",
    "# Input tensors\n",
    "dummy_input_ids = torch.ones(batch_size, sequence_length, dtype=torch.long, device=device)\n",
    "dummy_attention_mask = torch.ones(batch_size, sequence_length, dtype=torch.long, device=device)\n",
    "dummy_position_ids = torch.arange(sequence_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "dummy_past_key_values = None  # Placeholder for past key values\n",
    "dummy_cache_position = None  # Placeholder for cache position\n",
    "\n",
    "# Define dynamic axes for variable sequence lengths\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'position_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'logits': {0: 'batch_size', 1: 'sequence_length'}\n",
    "}\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    wrapped_model,\n",
    "    (\n",
    "        dummy_input_ids,\n",
    "        dummy_attention_mask,\n",
    "        dummy_position_ids,\n",
    "        dummy_past_key_values,\n",
    "        dummy_cache_position\n",
    "    ),\n",
    "    '/kaggle/working/tiny_llama.onnx',\n",
    "    input_names=['input_ids', 'attention_mask', 'position_ids', 'past_key_values', 'cache_position'],\n",
    "    output_names=['logits', 'past_key_values'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=18, \n",
    "    do_constant_folding=False,\n",
    "    verbose=False \n",
    ")\n",
    "2. Inference Code for Token-by-Token Generation\n",
    "Issues in Your Inference Code:\n",
    "Cache Handling: You need to handle the cache properly for each token generation step.\n",
    "Dynamic Inputs: Ensure that inputs are handled dynamically for each token.\n",
    "Revised Inference Code:\n",
    "\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "\n",
    "ort_session = ort.InferenceSession(\"/kaggle/working/tiny_llama.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "def prepare_inputs(text_input, tokenizer, target_length=256, device='cuda'):\n",
    "    \"\"\"\n",
    "    Prepare inputs by padding/truncating to target length\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    encoded = tokenizer.encode(text_input, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = encoded[0]  # Remove batch dimension\n",
    "    \n",
    "    # Get the actual sequence length\n",
    "    seq_len = len(input_ids)\n",
    "    \n",
    "    if seq_len > target_length:\n",
    "        # Truncate if longer than target_length\n",
    "        input_ids = input_ids[:target_length]\n",
    "        seq_len = target_length\n",
    "    \n",
    "    # Pad input_ids\n",
    "    padding_length = target_length - seq_len\n",
    "    padding_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    input_ids = torch.cat([\n",
    "        input_ids,\n",
    "        torch.full((padding_length,), padding_id, dtype=torch.long)\n",
    "    ])\n",
    "    \n",
    "    # Create attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = torch.cat([\n",
    "        torch.ones(seq_len, dtype=torch.long),  # Explicitly set dtype to long\n",
    "        torch.zeros(padding_length, dtype=torch.long)  # Explicitly set dtype to long\n",
    "    ])\n",
    "    \n",
    "    # Create position ids\n",
    "    position_ids = torch.arange(target_length, dtype=torch.long)  # Explicitly set dtype to long\n",
    "\n",
    "    # Add batch dimension and move to device\n",
    "    input_ids = input_ids.unsqueeze(0).to(device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(device)\n",
    "    position_ids = position_ids.unsqueeze(0).to(device)\n",
    "    \n",
    "    return input_ids, attention_mask, position_ids\n",
    "\n",
    "def generate_tokens(prompt: str, tokenizer, num_tokens_to_generate: int = 40, device='cuda'):\n",
    "    input_ids, attention_mask, position_ids = prepare_inputs(prompt, tokenizer, target_length=256, device=device)\n",
    "    past_key_values = None\n",
    "    cache_position = torch.tensor([len(input_ids[0])], device=device)\n",
    "\n",
    "    for _ in range(num_tokens_to_generate):\n",
    "        ort_inputs = { \n",
    "            ort_session.get_inputs()[0].name: to_numpy(input_ids),\n",
    "            ort_session.get_inputs()[1].name: to_numpy(attention_mask),\n",
    "            ort_session.get_inputs()[2].name: to_numpy(position_ids),\n",
    "            ort_session.get_inputs()[3].name: to_numpy(past_key_values) if past_key_values is not None else None,\n",
    "            ort_session.get_inputs()[4].name: to_numpy(cache_position)\n",
    "        }\n",
    "        ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "        logits = torch.from_numpy(ort_outs[0]).to(device)\n",
    "        past_key_values = ort_outs[1]\n",
    "\n",
    "        next_token = torch.argmax(logits[:, -1], dim=-1)[:, None]\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones((1, 1), dtype=torch.long, device=device)], dim=-1)\n",
    "        position_ids = torch.cat([position_ids, torch.tensor([[cache_position.item()]], device=device)], dim=-1)\n",
    "        cache_position += 1\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Once upon a time\"\n",
    "generated_text = generate_tokens(prompt, tokenizer, num_tokens_to_generate=40, device='cuda')\n",
    "print(generated_text)\n",
    "This revised code should help you generate text token by token using the ONNX model. Make sure to adjust paths and parameters according to your specific setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! Let's clarify the concepts and ensure that the implementation aligns with your goal of using a static cache for faster inference.\n",
    "\n",
    "Understanding Static Cache vs. Dynamic Cache\n",
    "Dynamic Cache:\n",
    "Purpose: Automatically manages the cache for each forward pass. Suitable for single-pass inference where the cache is not reused across multiple calls.\n",
    "Usage: Typically used when you don't need to persist the cache between different inference calls.\n",
    "Static Cache:\n",
    "Purpose: Persists the cache across multiple forward passes. Useful for generating sequences token by token, where the cache needs to be reused.\n",
    "Usage: Requires manual management of the cache, ensuring that the cache is updated and reused correctly.\n",
    "Using Static Cache in Hugging Face Transformers\n",
    "The StaticCache class you mentioned is part of Hugging Face's transformers library and is designed to manage the cache statically. However, when exporting to ONNX, you need to ensure that the cache is handled correctly in the ONNX model.\n",
    "\n",
    "Revised Implementation\n",
    "Let's revise the implementation to ensure that the static cache is used correctly both in the PyTorch model and during ONNX inference.\n",
    "\n",
    "1. Model Wrapper with Static Cache\n",
    "First, ensure that the StaticCache class is correctly defined and used. Here’s how you can modify the ModelWrapper to use StaticCache.\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import PreTrainedModel, LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers.generation.utils import StaticCache\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.static_cache = None\n",
    "        \n",
    "    def initialize_cache(self, batch_size, max_cache_len, device, dtype):\n",
    "        self.static_cache = StaticCache(\n",
    "            config=self.model.config,\n",
    "            max_batch_size=batch_size,\n",
    "            max_cache_len=max_cache_len,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, position_ids, cache_position=None):\n",
    "        if self.static_cache is None:\n",
    "            raise ValueError(\"Static cache has not been initialized.\")\n",
    "        \n",
    "        # Forward pass with static cache\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=self.static_cache.past_key_values,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=True,\n",
    "            return_dict=False\n",
    "        )\n",
    "        \n",
    "        # Update the static cache with the new past key values\n",
    "        self.static_cache.update(outputs[1])\n",
    "        \n",
    "        return outputs[0]  # Return only logits for simplicity\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model = LlamaForCausalLM.from_pretrained('path_to_your_model').to(device)\n",
    "tokenizer = LlamaTokenizer.from_pretrained('path_to_your_model')\n",
    "\n",
    "# Initialize the model wrapper\n",
    "wrapped_model = ModelWrapper(model).to(device)\n",
    "\n",
    "# Initialize the static cache\n",
    "batch_size = 1  # Adjust as needed\n",
    "max_cache_len = 256  # Adjust as needed\n",
    "wrapped_model.initialize_cache(batch_size, max_cache_len, device, model.dtype)\n",
    "\n",
    "# Dummy inputs for ONNX export\n",
    "dummy_input_ids = torch.ones(batch_size, 1, dtype=torch.long, device=device)\n",
    "dummy_attention_mask = torch.ones(batch_size, 1, dtype=torch.long, device=device)\n",
    "dummy_position_ids = torch.tensor([[0]], dtype=torch.long, device=device)\n",
    "dummy_cache_position = torch.tensor([0], dtype=torch.long, device=device)\n",
    "\n",
    "# Define dynamic axes for variable sequence lengths\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'position_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'logits': {0: 'batch_size', 1: 'sequence_length'}\n",
    "}\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    wrapped_model,\n",
    "    (\n",
    "        dummy_input_ids,\n",
    "        dummy_attention_mask,\n",
    "        dummy_position_ids,\n",
    "        dummy_cache_position\n",
    "    ),\n",
    "    '/kaggle/working/tiny_llama.onnx',\n",
    "    input_names=['input_ids', 'attention_mask', 'position_ids', 'cache_position'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=18, \n",
    "    do_constant_folding=False,\n",
    "    verbose=False \n",
    ")\n",
    "2. Inference Code with Static Cache\n",
    "Now, let's modify the inference code to handle the static cache correctly.\n",
    "\n",
    "\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "\n",
    "ort_session = ort.InferenceSession(\"/kaggle/working/tiny_llama.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "def prepare_initial_inputs(text_input, tokenizer, device='cuda'):\n",
    "    \"\"\"\n",
    "    Prepare initial inputs by tokenizing the prompt\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    encoded = tokenizer.encode(text_input, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = encoded[0]  # Remove batch dimension\n",
    "    \n",
    "    # Create attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "    \n",
    "    # Create position ids\n",
    "    position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
    "    \n",
    "    # Add batch dimension and move to device\n",
    "    input_ids = input_ids.unsqueeze(0).to(device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(device)\n",
    "    position_ids = position_ids.to(device)\n",
    "    \n",
    "    return input_ids, attention_mask, position_ids\n",
    "\n",
    "def generate_tokens(prompt: str, tokenizer, num_tokens_to_generate: int = 40, device='cuda'):\n",
    "    input_ids, attention_mask, position_ids = prepare_initial_inputs(prompt, tokenizer, device=device)\n",
    "    cache_position = torch.tensor([len(input_ids[0])], dtype=torch.long, device=device)\n",
    "    \n",
    "    for _ in range(num_tokens_to_generate):\n",
    "        ort_inputs = { \n",
    "            ort_session.get_inputs()[0].name: to_numpy(input_ids),\n",
    "            ort_session.get_inputs()[1].name: to_numpy(attention_mask),\n",
    "            ort_session.get_inputs()[2].name: to_numpy(position_ids),\n",
    "            ort_session.get_inputs()[3].name: to_numpy(cache_position)\n",
    "        }\n",
    "        ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "        logits = torch.from_numpy(ort_outs[0]).to(device)\n",
    "        \n",
    "        next_token = torch.argmax(logits[:, -1], dim=-1)[:, None]\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones((1, 1), dtype=torch.long, device=device)], dim=-1)\n",
    "        position_ids = torch.cat([position_ids, torch.tensor([[cache_position.item()]], device=device)], dim=-1)\n",
    "        cache_position += 1\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "# prompt = \"Once upon a time\"\n",
    "# generated_text = generate_tokens(prompt, tokenizer, num_tokens_to_generate=40, device='cuda')\n",
    "# print(generated_text)\n",
    "# Key Points\n",
    "\n",
    "# Static Cache Initialization:\n",
    "# The StaticCache is initialized once and reused across multiple inference steps.\n",
    "# This ensures that the cache is managed efficiently and persists between token generations.\n",
    "# Dynamic Inputs:\n",
    "# The input_ids, attention_mask, and position_ids are dynamically updated for each token generation step.\n",
    "# The cache_position is incremented to reflect the current position in the sequence.\n",
    "# ONNX Export:\n",
    "# The ONNX model is exported with the necessary inputs (input_ids, attention_mask, position_ids, cache_position) and outputs (logits).\n",
    "# The dynamic axes ensure that the model can handle variable sequence lengths.\n",
    "# By following this approach, you should be able to achieve efficient token-by-token generation using a static cache in both PyTorch and ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
