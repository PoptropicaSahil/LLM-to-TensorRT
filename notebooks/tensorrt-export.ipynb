{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89d5060",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-15T00:54:48.424448Z",
     "iopub.status.busy": "2024-11-15T00:54:48.423921Z",
     "iopub.status.idle": "2024-11-15T00:54:49.427027Z",
     "shell.execute_reply": "2024-11-15T00:54:49.425698Z"
    },
    "papermill": {
     "duration": 1.018794,
     "end_time": "2024-11-15T00:54:49.429951",
     "exception": false,
     "start_time": "2024-11-15T00:54:48.411157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input/'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fbef4",
   "metadata": {
    "papermill": {
     "duration": 0.010152,
     "end_time": "2024-11-15T00:54:49.450429",
     "exception": false,
     "start_time": "2024-11-15T00:54:49.440277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30a30fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:54:49.472691Z",
     "iopub.status.busy": "2024-11-15T00:54:49.472146Z",
     "iopub.status.idle": "2024-11-15T00:56:55.296473Z",
     "shell.execute_reply": "2024-11-15T00:56:55.295139Z"
    },
    "papermill": {
     "duration": 125.838643,
     "end_time": "2024-11-15T00:56:55.299464",
     "exception": false,
     "start_time": "2024-11-15T00:54:49.460821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorrt\r\n",
      "  Downloading tensorrt-10.6.0.tar.gz (16 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting onnxruntime\r\n",
      "  Downloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\r\n",
      "Collecting tensorrt-cu12==10.6.0 (from tensorrt)\r\n",
      "  Downloading tensorrt-cu12-10.6.0.tar.gz (18 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hCollecting coloredlogs (from onnxruntime)\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (24.3.25)\r\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (21.3)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (3.20.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (1.12)\r\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->onnxruntime) (3.1.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\r\n",
      "Downloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: tensorrt, tensorrt-cu12\r\n",
      "  Building wheel for tensorrt (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for tensorrt: filename=tensorrt-10.6.0-py2.py3-none-any.whl size=16337 sha256=f94c6de65adf0c7406ff3136d2d5eb6b3757d779a7d379d801e3341a980dd3c9\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/97/95/8fdbe17369eed28ee9903523b94e130023ae58edcf1c904e5a\r\n",
      "  Building wheel for tensorrt-cu12 (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for tensorrt-cu12: filename=tensorrt_cu12-10.6.0-py2.py3-none-any.whl size=17551 sha256=1b359eb40cfee7ee00f55fea087936daad074006e67985c815c5faf357e9da3b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/78/db/ca/0e81d122890b48aaa016992e062a1eb16dc3d00ac546156517\r\n",
      "Successfully built tensorrt tensorrt-cu12\r\n",
      "Installing collected packages: tensorrt-cu12, humanfriendly, tensorrt, coloredlogs, onnxruntime\r\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.20.0 tensorrt-10.6.0 tensorrt-cu12-10.6.0\r\n"
     ]
    }
   ],
   "source": [
    "# Installed to a local directory and now I keep adding the notebook to this work\n",
    "\n",
    "!pip install tensorrt onnxruntime # --quiet\n",
    "# !pip install --upgrade tensorrt onnxruntime --target=/kaggle/working/mysitepackages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9208501b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:56:55.387700Z",
     "iopub.status.busy": "2024-11-15T00:56:55.387231Z",
     "iopub.status.idle": "2024-11-15T00:57:00.620292Z",
     "shell.execute_reply": "2024-11-15T00:57:00.618987Z"
    },
    "papermill": {
     "duration": 5.279782,
     "end_time": "2024-11-15T00:57:00.623127",
     "exception": false,
     "start_time": "2024-11-15T00:56:55.343345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import transformers\n",
    "# from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.onnx import OnnxConfig\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fef6a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:57:00.711289Z",
     "iopub.status.busy": "2024-11-15T00:57:00.709836Z",
     "iopub.status.idle": "2024-11-15T00:57:00.795791Z",
     "shell.execute_reply": "2024-11-15T00:57:00.794230Z"
    },
    "papermill": {
     "duration": 0.13336,
     "end_time": "2024-11-15T00:57:00.799106",
     "exception": false,
     "start_time": "2024-11-15T00:57:00.665746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22478f97",
   "metadata": {
    "papermill": {
     "duration": 0.043158,
     "end_time": "2024-11-15T00:57:00.885392",
     "exception": false,
     "start_time": "2024-11-15T00:57:00.842234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Covert to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a9a7bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:57:00.975286Z",
     "iopub.status.busy": "2024-11-15T00:57:00.974798Z",
     "iopub.status.idle": "2024-11-15T00:57:02.137482Z",
     "shell.execute_reply": "2024-11-15T00:57:02.135688Z"
    },
    "papermill": {
     "duration": 1.211066,
     "end_time": "2024-11-15T00:57:02.140757",
     "exception": false,
     "start_time": "2024-11-15T00:57:00.929691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Important to do this step in Kaggle only while creating :/\n",
    "! touch tiny_llama.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d57523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:57:02.230397Z",
     "iopub.status.busy": "2024-11-15T00:57:02.229218Z",
     "iopub.status.idle": "2024-11-15T00:57:06.654956Z",
     "shell.execute_reply": "2024-11-15T00:57:06.653444Z"
    },
    "papermill": {
     "duration": 4.473768,
     "end_time": "2024-11-15T00:57:06.657668",
     "exception": false,
     "start_time": "2024-11-15T00:57:02.183900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "device='cpu' # 'cuda'\n",
    "\n",
    "model_path = \"/kaggle/input/m/mambagetout/tinyllama/pytorch/default/1/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token # added because of generate thing\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e48100ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:57:06.746575Z",
     "iopub.status.busy": "2024-11-15T00:57:06.745841Z",
     "iopub.status.idle": "2024-11-15T00:57:06.752278Z",
     "shell.execute_reply": "2024-11-15T00:57:06.750904Z"
    },
    "papermill": {
     "duration": 0.054014,
     "end_time": "2024-11-15T00:57:06.755041",
     "exception": false,
     "start_time": "2024-11-15T00:57:06.701027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import StaticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c130f9fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:57:06.843380Z",
     "iopub.status.busy": "2024-11-15T00:57:06.842869Z",
     "iopub.status.idle": "2024-11-15T00:57:06.850113Z",
     "shell.execute_reply": "2024-11-15T00:57:06.848868Z"
    },
    "papermill": {
     "duration": 0.055209,
     "end_time": "2024-11-15T00:57:06.853222",
     "exception": false,
     "start_time": "2024-11-15T00:57:06.798013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dummy inputs\n",
    "batch_size = 1\n",
    "sequence_length = 256  # Reduced for testing\n",
    "\n",
    "# Get model configuration\n",
    "num_layers = model.config.num_hidden_layers\n",
    "num_attention_heads = model.config.num_attention_heads\n",
    "hidden_size = model.config.hidden_size\n",
    "head_dim = hidden_size // num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e489a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:57:06.948574Z",
     "iopub.status.busy": "2024-11-15T00:57:06.947138Z",
     "iopub.status.idle": "2024-11-15T00:57:06.981431Z",
     "shell.execute_reply": "2024-11-15T00:57:06.980030Z"
    },
    "papermill": {
     "duration": 0.086319,
     "end_time": "2024-11-15T00:57:06.984555",
     "exception": false,
     "start_time": "2024-11-15T00:57:06.898236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, position_ids):\n",
    "        # Initialize static cache\n",
    "        past_key_values = StaticCache(\n",
    "            config=self.model.config,\n",
    "            batch_size=batch_size,\n",
    "            max_cache_len= sequence_length, # 32,  # Match the sequence length\n",
    "            device=input_ids.device,\n",
    "            dtype=self.model.dtype\n",
    "        )\n",
    "        \n",
    "        # Generate cache position\n",
    "        cache_position = torch.arange(sequence_length, device=input_ids.device)\n",
    "        \n",
    "        # Forward pass with static cache\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            cache_position=cache_position, # Can keep None also? TRYING FOR NOW\n",
    "            use_cache=True,\n",
    "            return_dict=False\n",
    "        )\n",
    "        \n",
    "        return outputs[0]  # Return only logits for simplicity\n",
    "\n",
    "# Wrap the model, keeping on gpu is essestial else \n",
    "# error two devices found cuda and cuda:0 error\n",
    "# wrapped_model = ModelWrapper(model).cuda()\n",
    "wrapped_model = ModelWrapper(model).to(device)\n",
    "\n",
    "# Input tensors\n",
    "dummy_input_ids = torch.ones(batch_size, sequence_length, dtype=torch.long, device=device)\n",
    "dummy_attention_mask = torch.ones(batch_size, sequence_length, dtype=torch.long, device=device)\n",
    "dummy_position_ids = torch.arange(sequence_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "# Define dynamic axes for variable sequence lengths\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'position_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50316b78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:57:07.072040Z",
     "iopub.status.busy": "2024-11-15T00:57:07.071485Z",
     "iopub.status.idle": "2024-11-15T00:59:00.453674Z",
     "shell.execute_reply": "2024-11-15T00:59:00.452144Z"
    },
    "papermill": {
     "duration": 113.430091,
     "end_time": "2024-11-15T00:59:00.457131",
     "exception": false,
     "start_time": "2024-11-15T00:57:07.027040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:97: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "# Export the model\n",
    "# using cpu\n",
    "\n",
    "torch.onnx.export(\n",
    "    wrapped_model,\n",
    "    (\n",
    "        dummy_input_ids,\n",
    "        dummy_attention_mask,\n",
    "        dummy_position_ids,\n",
    "    ),\n",
    "    # '/kaggle/working/tiny_llama.onnx',\n",
    "    '/kaggle/working/tiny_llama.onnx',\n",
    "    input_names=['input_ids', 'attention_mask', 'position_ids'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=18, # DO NOT, I REPEAT DO NOT USE OPSET_VERSION 15. FULL PAIN IT ISSSS.\n",
    "    # do_constant_folding=False, # True, # CHECKING ANOTHER OPTION because later error. Depreciated only!\n",
    "    verbose=False,\n",
    "\n",
    "    # Extra params because of export exceeding size issue\n",
    "    export_params=True, # IMP\n",
    "    # export_type=torch.onnx.ExportTypes.PROTOBUF_FILE,\n",
    "    \n",
    "    # Specify external data parameters\n",
    "    # storage_format=torch.onnx.FileFormat.EXTERNAL,\n",
    "    # external_data_format=torch.onnx.FileFormat.PROTOBUF,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55023047",
   "metadata": {
    "papermill": {
     "duration": 0.059367,
     "end_time": "2024-11-15T00:59:00.647707",
     "exception": false,
     "start_time": "2024-11-15T00:59:00.588340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6d12f9f",
   "metadata": {
    "papermill": {
     "duration": 0.056037,
     "end_time": "2024-11-15T00:59:00.758881",
     "exception": false,
     "start_time": "2024-11-15T00:59:00.702844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **load the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1539f99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:00.876749Z",
     "iopub.status.busy": "2024-11-15T00:59:00.875991Z",
     "iopub.status.idle": "2024-11-15T00:59:00.887955Z",
     "shell.execute_reply": "2024-11-15T00:59:00.886522Z"
    },
    "papermill": {
     "duration": 0.07347,
     "end_time": "2024-11-15T00:59:00.890284",
     "exception": false,
     "start_time": "2024-11-15T00:59:00.816814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/tiny_llama.onnx\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        if '.onnx' in filename:\n",
    "            print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aa9562d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:00.978404Z",
     "iopub.status.busy": "2024-11-15T00:59:00.977956Z",
     "iopub.status.idle": "2024-11-15T00:59:06.068236Z",
     "shell.execute_reply": "2024-11-15T00:59:06.066966Z"
    },
    "papermill": {
     "duration": 5.137974,
     "end_time": "2024-11-15T00:59:06.071211",
     "exception": false,
     "start_time": "2024-11-15T00:59:00.933237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"/kaggle/working/tiny_llama.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25d4e08e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:06.165407Z",
     "iopub.status.busy": "2024-11-15T00:59:06.164961Z",
     "iopub.status.idle": "2024-11-15T00:59:06.172906Z",
     "shell.execute_reply": "2024-11-15T00:59:06.171627Z"
    },
    "papermill": {
     "duration": 0.058067,
     "end_time": "2024-11-15T00:59:06.175351",
     "exception": false,
     "start_time": "2024-11-15T00:59:06.117284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd436930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:06.267116Z",
     "iopub.status.busy": "2024-11-15T00:59:06.266123Z",
     "iopub.status.idle": "2024-11-15T00:59:06.271379Z",
     "shell.execute_reply": "2024-11-15T00:59:06.270176Z"
    },
    "papermill": {
     "duration": 0.053827,
     "end_time": "2024-11-15T00:59:06.273874",
     "exception": false,
     "start_time": "2024-11-15T00:59:06.220047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# Might give\n",
    "# ValueError: Message onnx.ModelProto exceeds maximum protobuf size of 2GB: 4402590179\n",
    "# But this is protobuf's mistake not ours xD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d74f45f",
   "metadata": {
    "papermill": {
     "duration": 0.044377,
     "end_time": "2024-11-15T00:59:06.362787",
     "exception": false,
     "start_time": "2024-11-15T00:59:06.318410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run Inference using the ort InferenceSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab3c3812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:06.455107Z",
     "iopub.status.busy": "2024-11-15T00:59:06.454316Z",
     "iopub.status.idle": "2024-11-15T00:59:12.065623Z",
     "shell.execute_reply": "2024-11-15T00:59:12.064330Z"
    },
    "papermill": {
     "duration": 5.660007,
     "end_time": "2024-11-15T00:59:12.068303",
     "exception": false,
     "start_time": "2024-11-15T00:59:06.408296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(\n",
    "    \"/kaggle/working/tiny_llama.onnx\", \n",
    "    providers=[\"CPUExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7e87058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:12.158703Z",
     "iopub.status.busy": "2024-11-15T00:59:12.158210Z",
     "iopub.status.idle": "2024-11-15T00:59:17.172591Z",
     "shell.execute_reply": "2024-11-15T00:59:17.171260Z"
    },
    "papermill": {
     "duration": 5.063185,
     "end_time": "2024-11-15T00:59:17.176055",
     "exception": false,
     "start_time": "2024-11-15T00:59:12.112870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "ort_inputs = {\n",
    "    ort_session.get_inputs()[0].name: to_numpy(dummy_input_ids),\n",
    "    ort_session.get_inputs()[1].name: to_numpy(dummy_attention_mask),\n",
    "    ort_session.get_inputs()[2].name: to_numpy(dummy_position_ids)\n",
    "}\n",
    "\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3844a2b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:17.272443Z",
     "iopub.status.busy": "2024-11-15T00:59:17.271878Z",
     "iopub.status.idle": "2024-11-15T00:59:17.280943Z",
     "shell.execute_reply": "2024-11-15T00:59:17.279810Z"
    },
    "papermill": {
     "duration": 0.060761,
     "end_time": "2024-11-15T00:59:17.283962",
     "exception": false,
     "start_time": "2024-11-15T00:59:17.223201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 1, (1, 256, 32000))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ort_outs), len(ort_outs), ort_outs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6a8bb63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:17.384553Z",
     "iopub.status.busy": "2024-11-15T00:59:17.384020Z",
     "iopub.status.idle": "2024-11-15T00:59:23.583237Z",
     "shell.execute_reply": "2024-11-15T00:59:23.581965Z"
    },
    "papermill": {
     "duration": 6.251533,
     "end_time": "2024-11-15T00:59:23.585991",
     "exception": false,
     "start_time": "2024-11-15T00:59:17.334458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if outputs are close\n",
    "np.testing.assert_allclose(\n",
    "    to_numpy(wrapped_model(dummy_input_ids, dummy_attention_mask, dummy_position_ids)), \n",
    "    ort_outs[0], \n",
    "    rtol=1e-03, \n",
    "    atol=1e-04\n",
    ")\n",
    "# throws assertion error when atol is 1e-05\n",
    "# So I think this is fair enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0ff79",
   "metadata": {
    "papermill": {
     "duration": 0.042931,
     "end_time": "2024-11-15T00:59:23.671944",
     "exception": false,
     "start_time": "2024-11-15T00:59:23.629013",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## this works, so now work with text inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb95d4",
   "metadata": {
    "papermill": {
     "duration": 0.04883,
     "end_time": "2024-11-15T00:59:23.763817",
     "exception": false,
     "start_time": "2024-11-15T00:59:23.714987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Check the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78480e32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:23.853153Z",
     "iopub.status.busy": "2024-11-15T00:59:23.852678Z",
     "iopub.status.idle": "2024-11-15T00:59:23.867020Z",
     "shell.execute_reply": "2024-11-15T00:59:23.865524Z"
    },
    "papermill": {
     "duration": 0.062115,
     "end_time": "2024-11-15T00:59:23.869663",
     "exception": false,
     "start_time": "2024-11-15T00:59:23.807548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_input_ids = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) \n",
      " dummy_attention_mask = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) \n",
      " dummy_position_ids = tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "         168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "         182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "         210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "         224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "         252, 253, 254, 255]])\n"
     ]
    }
   ],
   "source": [
    "print(f'dummy_input_ids = {dummy_input_ids} \\n dummy_attention_mask = {dummy_attention_mask} \\n dummy_position_ids = {dummy_position_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3e4bbde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:23.974199Z",
     "iopub.status.busy": "2024-11-15T00:59:23.973080Z",
     "iopub.status.idle": "2024-11-15T00:59:23.986293Z",
     "shell.execute_reply": "2024-11-15T00:59:23.985111Z"
    },
    "papermill": {
     "duration": 0.076336,
     "end_time": "2024-11-15T00:59:23.989456",
     "exception": false,
     "start_time": "2024-11-15T00:59:23.913120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_inputs(text_input, tokenizer, target_length=256, device='cuda'):\n",
    "    \"\"\"\n",
    "    Prepare inputs by padding/truncating to target length\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    encoded = tokenizer.encode(text_input, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = encoded[0]  # Remove batch dimension\n",
    "    \n",
    "    # Get the actual sequence length\n",
    "    seq_len = len(input_ids)\n",
    "    \n",
    "    if seq_len > target_length:\n",
    "        # Truncate if longer than target_length\n",
    "        input_ids = input_ids[:target_length]\n",
    "        seq_len = target_length\n",
    "    \n",
    "    # Pad input_ids\n",
    "    padding_length = target_length - seq_len\n",
    "    padding_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    input_ids = torch.cat([\n",
    "        input_ids,\n",
    "        torch.full((padding_length,), padding_id, dtype=torch.long)\n",
    "    ])\n",
    "    \n",
    "    # Create attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = torch.cat([\n",
    "        torch.ones(seq_len, dtype=torch.long),  # Explicitly set dtype to long\n",
    "        torch.zeros(padding_length, dtype=torch.long)  # Explicitly set dtype to long\n",
    "    ])\n",
    "    \n",
    "    # Create position ids\n",
    "    # position_ids = torch.arange(target_length)\n",
    "    position_ids = torch.arange(target_length, dtype=torch.long)  # Explicitly set dtype to long\n",
    "\n",
    "    \n",
    "    # Add batch dimension and move to device\n",
    "    input_ids = input_ids.unsqueeze(0).to(device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(device)\n",
    "    position_ids = position_ids.unsqueeze(0).to(device)\n",
    "    \n",
    "    return input_ids, attention_mask, position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7dc411b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:24.086716Z",
     "iopub.status.busy": "2024-11-15T00:59:24.086291Z",
     "iopub.status.idle": "2024-11-15T00:59:24.094423Z",
     "shell.execute_reply": "2024-11-15T00:59:24.093214Z"
    },
    "papermill": {
     "duration": 0.056458,
     "end_time": "2024-11-15T00:59:24.096975",
     "exception": false,
     "start_time": "2024-11-15T00:59:24.040517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_both_inferences(text_input: str):\n",
    "\n",
    "    input_ids, attention_mask, position_ids = prepare_inputs(\n",
    "        text_input, \n",
    "        tokenizer, \n",
    "        target_length=256,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    wrapped_model_outputs = to_numpy(wrapped_model(input_ids, attention_mask, position_ids))\n",
    "\n",
    "    ort_inputs = { \n",
    "        ort_session.get_inputs()[0].name: to_numpy(input_ids),\n",
    "        ort_session.get_inputs()[1].name: to_numpy(attention_mask),\n",
    "        ort_session.get_inputs()[2].name: to_numpy(position_ids)\n",
    "    }\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "    return wrapped_model_outputs, ort_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79901f60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:24.185510Z",
     "iopub.status.busy": "2024-11-15T00:59:24.185063Z",
     "iopub.status.idle": "2024-11-15T00:59:33.653193Z",
     "shell.execute_reply": "2024-11-15T00:59:33.651995Z"
    },
    "papermill": {
     "duration": 9.515996,
     "end_time": "2024-11-15T00:59:33.656266",
     "exception": false,
     "start_time": "2024-11-15T00:59:24.140270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wrapped_model_outputs, ort_outs = make_both_inferences(text_input ='hi there my work is computer science and I like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89eb322a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:33.747883Z",
     "iopub.status.busy": "2024-11-15T00:59:33.747403Z",
     "iopub.status.idle": "2024-11-15T00:59:33.758820Z",
     "shell.execute_reply": "2024-11-15T00:59:33.757857Z"
    },
    "papermill": {
     "duration": 0.059488,
     "end_time": "2024-11-15T00:59:33.761592",
     "exception": false,
     "start_time": "2024-11-15T00:59:33.702104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ -7.396678  ,  -7.3966794 ,   8.475743  , ...,  -4.358731  ,\n",
       "           -7.0164227 ,  -0.75330424],\n",
       "         [ -6.5529804 ,  -6.5529814 ,   7.802591  , ...,  -4.382573  ,\n",
       "           -6.0285573 ,  -0.10475935],\n",
       "         [ -6.6338015 ,  -6.6337953 ,   8.605309  , ...,  -4.369956  ,\n",
       "           -5.029665  ,   0.3729149 ],\n",
       "         ...,\n",
       "         [-10.389168  , -10.38916   ,   4.1876626 , ...,  -5.8836117 ,\n",
       "           -7.9302125 ,  -3.226597  ],\n",
       "         [-10.757662  , -10.757653  ,   3.5645452 , ...,  -5.9962487 ,\n",
       "           -7.8819776 ,  -3.4362864 ],\n",
       "         [-11.012067  , -11.012054  ,   3.273897  , ...,  -6.005757  ,\n",
       "           -7.9752884 ,  -3.61273   ]]], dtype=float32),\n",
       " [array([[[ -7.3966737 ,  -7.3966737 ,   8.475735  , ...,  -4.3587313 ,\n",
       "            -7.01641   ,  -0.7532996 ],\n",
       "          [ -6.552985  ,  -6.5529885 ,   7.8025846 , ...,  -4.3825774 ,\n",
       "            -6.028565  ,  -0.10476124],\n",
       "          [ -6.6337914 ,  -6.633787  ,   8.605286  , ...,  -4.3699517 ,\n",
       "            -5.0296597 ,   0.37291405],\n",
       "          ...,\n",
       "          [-10.389168  , -10.389161  ,   4.187644  , ...,  -5.88361   ,\n",
       "            -7.9302197 ,  -3.226603  ],\n",
       "          [-10.757671  , -10.757661  ,   3.564526  , ...,  -5.996253  ,\n",
       "            -7.88199   ,  -3.4362998 ],\n",
       "          [-11.012066  , -11.0120535 ,   3.273889  , ...,  -6.0057554 ,\n",
       "            -7.9752936 ,  -3.612728  ]]], dtype=float32)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_model_outputs, ort_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d440a54e",
   "metadata": {
    "papermill": {
     "duration": 0.044076,
     "end_time": "2024-11-15T00:59:33.851424",
     "exception": false,
     "start_time": "2024-11-15T00:59:33.807348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The token ids converted back to text aren't beutiful though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58eb4c17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:59:33.950337Z",
     "iopub.status.busy": "2024-11-15T00:59:33.949256Z",
     "iopub.status.idle": "2024-11-15T00:59:33.999262Z",
     "shell.execute_reply": "2024-11-15T00:59:33.998044Z"
    },
    "papermill": {
     "duration": 0.108837,
     "end_time": "2024-11-15T00:59:34.003966",
     "exception": false,
     "start_time": "2024-11-15T00:59:33.895129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amedag. my. my. computer computer am I I I I I I I I I I computer I I I I computer computer computer I I I I I I I I I I computer computer computer computer I I I I I I I I computer computer I I I I I I I I I I I I computer computer computer I I I computer computer I I I I I I I I I I computer computer computer I I I I I I I I I I I I I I I I I I I I I I I computer I I I I I I I I I I I I I I I I I computer computer I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I computer computer I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I computer I I I I computer I I I I I I I I I I I I I I I I I I I I I I I',\n",
       " 'amedag. my. my. computer computer am I I I I I I I I I I computer I I I I computer computer computer I I I I I I I I I I computer computer computer computer I I I I I I I I computer computer I I I I I I I I I I I I computer computer computer I I I computer computer I I I I I I I I I I computer computer computer I I I I I I I I I I I I I I I I I I I I I I I computer I I I I I I I I I I I I I I I I I computer computer I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I computer computer I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I computer I I I I computer I I I I I I I I I I I I I I I I I I I I I I I')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Convert logits to token IDs (if necessary)\n",
    "wrapped_token_ids = torch.argmax(torch.from_numpy(wrapped_model_outputs), dim=-1)\n",
    "ort_token_ids = torch.argmax(torch.from_numpy(ort_outs[0]), dim=-1)  # Assuming first output is relevant\n",
    "\n",
    "# Decode token IDs back to text\n",
    "wrapped_text_output = tokenizer.decode(wrapped_token_ids[0], skip_special_tokens=True)\n",
    "ort_text_output = tokenizer.decode(ort_token_ids[0], skip_special_tokens=True)\n",
    "wrapped_text_output, ort_text_output"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 207202020,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 156963,
     "modelInstanceId": 134194,
     "sourceId": 157908,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 291.664919,
   "end_time": "2024-11-15T00:59:36.980006",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-15T00:54:45.315087",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
