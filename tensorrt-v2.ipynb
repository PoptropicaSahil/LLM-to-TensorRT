{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T02:10:50.071600Z",
     "iopub.status.busy": "2024-11-12T02:10:50.070066Z",
     "iopub.status.idle": "2024-11-12T02:10:50.148602Z",
     "shell.execute_reply": "2024-11-12T02:10:50.147349Z",
     "shell.execute_reply.started": "2024-11-12T02:10:50.071554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import tensorrt as trt\n",
    "\n",
    "from transformers import StaticCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covert to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T02:21:21.926268Z",
     "iopub.status.busy": "2024-11-12T02:21:21.925787Z",
     "iopub.status.idle": "2024-11-12T02:21:22.359107Z",
     "shell.execute_reply": "2024-11-12T02:21:22.357931Z",
     "shell.execute_reply.started": "2024-11-12T02:21:21.926223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device='cpu' \n",
    "\n",
    "model_path = \"/kaggle/input/m/mambagetout/tinyllama/pytorch/default/1/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export takes care of a static KV cache and handling dynamic inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T02:27:44.482077Z",
     "iopub.status.busy": "2024-11-12T02:27:44.480865Z",
     "iopub.status.idle": "2024-11-12T02:27:44.488520Z",
     "shell.execute_reply": "2024-11-12T02:27:44.487308Z",
     "shell.execute_reply.started": "2024-11-12T02:27:44.482007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create dummy inputs\n",
    "batch_size = 1\n",
    "sequence_length = 256  # Reduced for testing\n",
    "\n",
    "# Get model configuration\n",
    "num_layers = model.config.num_hidden_layers\n",
    "num_attention_heads = model.config.num_attention_heads\n",
    "hidden_size = model.config.hidden_size\n",
    "head_dim = hidden_size // num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T02:27:44.731306Z",
     "iopub.status.busy": "2024-11-12T02:27:44.730860Z",
     "iopub.status.idle": "2024-11-12T02:27:44.749468Z",
     "shell.execute_reply": "2024-11-12T02:27:44.748285Z",
     "shell.execute_reply.started": "2024-11-12T02:27:44.731263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, position_ids):\n",
    "        # Initialize static cache\n",
    "        past_key_values = StaticCache(\n",
    "            config=self.model.config,\n",
    "            batch_size=batch_size,\n",
    "            max_cache_len= sequence_length, # 32,  # Match the sequence length\n",
    "            device=input_ids.device,\n",
    "            dtype=self.model.dtype\n",
    "        )\n",
    "        \n",
    "        # Generate cache position\n",
    "        cache_position = torch.arange(sequence_length, device=input_ids.device)\n",
    "        \n",
    "        # Forward pass with static cache\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=True,\n",
    "            return_dict=False\n",
    "        )\n",
    "        \n",
    "        return outputs[0]  # Return only logits for simplicity\n",
    "\n",
    "\n",
    "wrapped_model = ModelWrapper(model).to(device)\n",
    "\n",
    "# Input tensors\n",
    "dummy_input_ids = torch.ones(batch_size, sequence_length, dtype=torch.long, device=device)\n",
    "dummy_attention_mask = torch.ones(batch_size, sequence_length, dtype=torch.long, device=device)\n",
    "dummy_position_ids = torch.arange(sequence_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "# Define dynamic axes for variable sequence lengths\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'position_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T02:27:47.482632Z",
     "iopub.status.busy": "2024-11-12T02:27:47.482152Z",
     "iopub.status.idle": "2024-11-12T02:28:51.466423Z",
     "shell.execute_reply": "2024-11-12T02:28:51.465133Z",
     "shell.execute_reply.started": "2024-11-12T02:27:47.482584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    wrapped_model,\n",
    "    (\n",
    "        dummy_input_ids,\n",
    "        dummy_attention_mask,\n",
    "        dummy_position_ids,\n",
    "    ),\n",
    "    '/kaggle/working/tiny_llama.onnx',\n",
    "    input_names=['input_ids', 'attention_mask', 'position_ids'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=18, # DO NOT, I REPEAT DO NOT USE OPSET_VERSION 15. FULL PAIN.\n",
    "    do_constant_folding=False, \n",
    "    verbose=False \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference using the ort InferenceSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T02:29:38.889806Z",
     "iopub.status.busy": "2024-11-12T02:29:38.889372Z",
     "iopub.status.idle": "2024-11-12T02:30:28.701259Z",
     "shell.execute_reply": "2024-11-12T02:30:28.699966Z",
     "shell.execute_reply.started": "2024-11-12T02:29:38.889750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(\"/kaggle/working/tiny_llama.onnx\", providers=[\"CPUExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T02:30:28.706330Z",
     "iopub.status.busy": "2024-11-12T02:30:28.705787Z",
     "iopub.status.idle": "2024-11-12T02:30:34.124086Z",
     "shell.execute_reply": "2024-11-12T02:30:34.122955Z",
     "shell.execute_reply.started": "2024-11-12T02:30:28.706276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "ort_inputs = {\n",
    "    ort_session.get_inputs()[0].name: to_numpy(dummy_input_ids),\n",
    "    ort_session.get_inputs()[1].name: to_numpy(dummy_attention_mask),\n",
    "    ort_session.get_inputs()[2].name: to_numpy(dummy_position_ids)\n",
    "}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T02:30:34.126549Z",
     "iopub.status.busy": "2024-11-12T02:30:34.126046Z",
     "iopub.status.idle": "2024-11-12T02:30:34.134605Z",
     "shell.execute_reply": "2024-11-12T02:30:34.133301Z",
     "shell.execute_reply.started": "2024-11-12T02:30:34.126473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 1, (1, 256, 32000))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ort_outs), len(ort_outs), ort_outs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outputs are close which is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T02:13:11.845240Z",
     "iopub.status.busy": "2024-11-12T02:13:11.844457Z",
     "iopub.status.idle": "2024-11-12T02:13:32.888964Z",
     "shell.execute_reply": "2024-11-12T02:13:32.887742Z",
     "shell.execute_reply.started": "2024-11-12T02:13:11.845186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "# check if outputs are close\n",
    "np.testing.assert_allclose(\n",
    "    to_numpy(wrapped_model(dummy_input_ids, dummy_attention_mask, dummy_position_ids)), \n",
    "    ort_outs[0], \n",
    "    rtol=1e-03, \n",
    "    atol=1e-04\n",
    ")\n",
    "# throws assertion error when atol is 1e-05\n",
    "# So I think this is fair enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this works, so now work with text inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T02:43:07.441852Z",
     "iopub.status.busy": "2024-11-12T02:43:07.441351Z",
     "iopub.status.idle": "2024-11-12T02:43:07.459770Z",
     "shell.execute_reply": "2024-11-12T02:43:07.458547Z",
     "shell.execute_reply.started": "2024-11-12T02:43:07.441797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_inputs(text_input, tokenizer, target_length=256, device='cuda'):\n",
    "    \"\"\"\n",
    "    Prepare inputs by padding/truncating to target length\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    encoded = tokenizer.encode(text_input, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = encoded[0]  # Remove batch dimension\n",
    "    \n",
    "    # Get the actual sequence length\n",
    "    seq_len = len(input_ids)\n",
    "    \n",
    "    if seq_len > target_length:\n",
    "        # Truncate if longer than target_length\n",
    "        input_ids = input_ids[:target_length]\n",
    "        seq_len = target_length\n",
    "    \n",
    "    # Pad input_ids\n",
    "    padding_length = target_length - seq_len\n",
    "    padding_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    input_ids = torch.cat([\n",
    "        input_ids,\n",
    "        torch.full((padding_length,), padding_id, dtype=torch.long)\n",
    "    ])\n",
    "    \n",
    "    # Create attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = torch.cat([\n",
    "        torch.ones(seq_len, dtype=torch.long),  # Explicitly set dtype to long\n",
    "        torch.zeros(padding_length, dtype=torch.long)  # Explicitly set dtype to long\n",
    "    ])\n",
    "    \n",
    "    # Create position ids\n",
    "    # position_ids = torch.arange(target_length)\n",
    "    position_ids = torch.arange(target_length, dtype=torch.long)  # Explicitly set dtype to long\n",
    "\n",
    "    \n",
    "    # Add batch dimension and move to device\n",
    "    input_ids = input_ids.unsqueeze(0).to(device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(device)\n",
    "    position_ids = position_ids.unsqueeze(0).to(device)\n",
    "    \n",
    "    return input_ids, attention_mask, position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T02:47:24.271751Z",
     "iopub.status.busy": "2024-11-12T02:47:24.269753Z",
     "iopub.status.idle": "2024-11-12T02:47:24.279983Z",
     "shell.execute_reply": "2024-11-12T02:47:24.278672Z",
     "shell.execute_reply.started": "2024-11-12T02:47:24.271699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_both_inferences(text_input: str):\n",
    "\n",
    "    input_ids, attention_mask, position_ids = prepare_inputs(\n",
    "        text_input, \n",
    "        tokenizer, \n",
    "        target_length=256,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    wrapped_model_outputs = to_numpy(wrapped_model(input_ids, attention_mask, position_ids))\n",
    "\n",
    "    ort_inputs = { \n",
    "        ort_session.get_inputs()[0].name: to_numpy(input_ids),\n",
    "        ort_session.get_inputs()[1].name: to_numpy(attention_mask),\n",
    "        ort_session.get_inputs()[2].name: to_numpy(position_ids)\n",
    "    }\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "    return wrapped_model_outputs, ort_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T03:01:13.743627Z",
     "iopub.status.busy": "2024-11-12T03:01:13.742573Z",
     "iopub.status.idle": "2024-11-12T03:01:25.803947Z",
     "shell.execute_reply": "2024-11-12T03:01:25.802656Z",
     "shell.execute_reply.started": "2024-11-12T03:01:13.743575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wrapped_model_outputs, ort_outs = make_both_inferences(text_input ='hi there my work is computer science and I like')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The token ids converted back to text aren't beautiful though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T03:01:48.802995Z",
     "iopub.status.busy": "2024-11-12T03:01:48.801749Z",
     "iopub.status.idle": "2024-11-12T03:01:48.853670Z",
     "shell.execute_reply": "2024-11-12T03:01:48.852446Z",
     "shell.execute_reply.started": "2024-11-12T03:01:48.802933Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amedag. my. my. computer computer am I I I my my my my my computer computer computer I I I computer computer computer computer my my my my I I my my my my computer computer computer computer computer computer andag my my my my computer computer computer computer my I\\n\\nag my my my my computer computer computer computer computer computer computer computer computer computer my computer I I my my my my my computer computer computer computer computer my my I my my my my my and computer my my and and andag my my my my computer computer computer my my my my I I I I I\\n\\n I my my my computer computer computer computer I I I\\n\\nag\\n my my my my computer computer computer computer I I I my my my computer computer computer computer my my my and and and my my my my computer computer my my I and and I my my computer computer\\n\\n\\n my my my my computer computer computer my I I I I my my my\\n\\n my my my my computer I I I my my my computer computer computer my my my and and and my my computer computer computeragag my my my computer computer computer computer computer computer computer\\n and and and my and and my my my my my and I my my my computer computer computer computer my my',\n",
       " 'amedag. my. my. computer computer am I I I my my my my my computer computer computer I I I computer computer computer computer my my my my I I my my my my computer computer computer computer computer computer andag my my my my computer computer computer computer my I\\n\\nag my my my my computer computer computer computer computer computer computer computer computer computer my computer I I my my my my my computer computer computer computer computer my my I my my my my my and computer my my and and andag my my my my computer computer computer my my my my I I I I I\\n\\n I my my my computer computer computer computer I I I\\n\\nag\\n my my my my computer computer computer computer I I I my my my computer computer computer computer my my my and and and my my my my computer computer my my I and and I my my computer computer\\n\\n\\n my my my my computer computer computer my I I I I my my my\\n\\n my my my my computer I I I my my my computer computer computer my my my and and and my my computer computer computeragag my my my computer computer computer computer computer computer computer\\n and and and my and and my my my my my and I my my my computer computer computer computer my my')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Convert logits to token IDs (if necessary)\n",
    "wrapped_token_ids = torch.argmax(torch.from_numpy(wrapped_model_outputs), dim=-1)\n",
    "ort_token_ids = torch.argmax(torch.from_numpy(ort_outs[0]), dim=-1)  # Assuming first output is relevant\n",
    "\n",
    "# Decode token IDs back to text\n",
    "wrapped_text_output = tokenizer.decode(wrapped_token_ids[0], skip_special_tokens=True)\n",
    "ort_text_output = tokenizer.decode(ort_token_ids[0], skip_special_tokens=True)\n",
    "wrapped_text_output, ort_text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook tensorrt-v2.ipynb to html\n",
      "[NbConvertApp] Writing 310840 bytes to tensorrt-v2.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html tensorrt-v2.ipynb"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 206552798,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 156963,
     "modelInstanceId": 134194,
     "sourceId": 157908,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
